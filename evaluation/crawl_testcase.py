import requests
import json
import re
import time

# â–›â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–œ
# â–ŒCONFIGURATION                            â–
# â–™â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–Ÿ
OUTPUT_FILE = "multilabel_dataset.json"
TARGET_PER_CATEGORY = 50

TAG_MAPPING = {
    "en:milk": "dairy food product",
    "en:eggs": "egg food product",
    "en:soybeans": "soybean food product",
    "en:peanuts": "peanut food product",
    "en:gluten": "wheat food product",
    "en:crustaceans": "shellfish food product",
    "en:fish": "fish food product",
    "en:nuts": "nut food product"
}

ENGLISH_COUNTRIES = ["en:united-states", "en:united-kingdom", "en:australia", "en:canada", "en:new-zealand"]

def clean_ingredients_text(raw_text):
    if not raw_text: return []
    text = raw_text.lower().split("contains:")[0]
    text = re.sub(r'\s*\d+([.,]\d+)?\s*%', '', text)
    text = text.replace("(", ",").replace(")", "").replace("[", "").replace("]", "").replace(".", "")
    items = [x.strip() for x in text.split(',') if x.strip()]
    return list(set([x for x in items if len(x) > 2]))

def is_english(text):
    return any(w in text.lower() for w in ["ingredients", "water", "sugar", "salt", "milk"])

def fetch_products(off_tag):
    products_found = []
    page = 1
    
    # Sá»¬A 1: TÄƒng giá»›i háº¡n trang lÃªn 20 hoáº·c 30 Ä‘á»ƒ Ä‘áº£m báº£o gom Ä‘á»§ hÃ ng
    MAX_PAGES = 30 
    
    while len(products_found) < TARGET_PER_CATEGORY and page <= MAX_PAGES:
        print(f"   â†³ Page {page}...", end="", flush=True)
        
        url = "https://world.openfoodfacts.org/cgi/search.pl"
        params = {
            "action": "process",
            "tagtype_0": "allergens",
            "tag_contains_0": "contains",
            "tag_0": off_tag,
            "json": "1",
            "page": page,
            "page_size": 100, # Láº¥y 100 mÃ³n má»—i láº§n gá»i
            "fields": "code,product_name,ingredients_text,allergens_tags,countries_tags"
        }
        
        try:
            resp = requests.get(url, params=params, timeout=10)
            
            # Sá»¬A 2: Xá»­ lÃ½ trÆ°á»ng há»£p API lá»—i hoáº·c háº¿t hÃ ng
            if resp.status_code != 200:
                print(f" (API Error {resp.status_code}) ", end="")
                break
                
            items = resp.json().get('products', [])
            if not items: 
                print(" (End of data) ", end="")
                break
            
            for p in items:
                # Náº¿u Ä‘Ã£ Ä‘á»§ chá»‰ tiÃªu thÃ¬ dá»«ng ngay láº­p tá»©c
                if len(products_found) >= TARGET_PER_CATEGORY: break
                
                raw_ing = p.get('ingredients_text', '')
                off_tags = p.get('allergens_tags', [])
                countries = p.get('countries_tags', [])

                # Filter cÆ¡ báº£n
                if not raw_ing or not is_english(raw_ing): continue
                if not any(c in ENGLISH_COUNTRIES for c in countries): continue

                # Chuáº©n hÃ³a Ground Truth
                true_allergens = []
                for tag in off_tags:
                    if tag in TAG_MAPPING:
                        true_allergens.append(TAG_MAPPING[tag])
                
                if not true_allergens: continue

                # ThÃªm vÃ o danh sÃ¡ch (Ä‘áº£m báº£o khÃ´ng trÃ¹ng ID trong list táº¡m nÃ y)
                current_ids = {p['id'] for p in products_found}
                p_id = p.get('code')
                
                if p_id not in current_ids:
                    products_found.append({
                        "id": p_id,
                        "product_name": p.get('product_name', 'Unknown'),
                        "scanned_ingredients": clean_ingredients_text(raw_ing),
                        "true_allergens": list(set(true_allergens))
                    })
            
            print(f" Got {len(products_found)}/{TARGET_PER_CATEGORY} ", end="")
            page += 1
            
            # Sá»¬A 3: Ngá»§ 1 xÃ­u Ä‘á»ƒ tÃ´n trá»ng Server
            time.sleep(0.5) 
            
        except Exception as e: 
            print(f" (Error: {e}) ", end="")
            break
            
    print("âœ…")
    return products_found

def main():
    print("ğŸš€ Báº®T Äáº¦U CRAWL DATA ÄA NHÃƒN...")
    final_db = {} # DÃ¹ng dict Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p sáº£n pháº©m
    
    for tag in TAG_MAPPING.keys():
        print(f"ğŸ“‚ Scanning {tag}...")
        items = fetch_products(tag)
        for item in items:
            final_db[item['id']] = item # Tá»± Ä‘á»™ng loáº¡i trÃ¹ng láº·p nhá» ID

    data_list = list(final_db.values())
    print(f"\nğŸ’¾ LÆ°u {len(data_list)} sáº£n pháº©m Ä‘á»™c nháº¥t vÃ o '{OUTPUT_FILE}'...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data_list, f, indent=4, ensure_ascii=False)

if __name__ == "__main__":
    main()